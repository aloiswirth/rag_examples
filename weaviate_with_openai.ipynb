{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJswABQ2a9ek"
      },
      "source": [
        "# RAG with Weaviate\n",
        "\n",
        "In this example we are using my personal openAI Subscription using the  \n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "jKvNCHOvQ0s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8D-ZEaa9en"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aloiswirth/rag_examples/blob/master/weaviate_with_openai.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the environment\n",
        "\n",
        "Check the environment we are in.\n",
        "\n",
        "Install all necessary libraries"
      ],
      "metadata": {
        "id": "UMtcpPdgtfxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython()"
      ],
      "metadata": {
        "id": "gFFQe9FZT_zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "Running_locally = 'ZMQInteractiveShell' in (str(get_ipython()))"
      ],
      "metadata": {
        "id": "pG17KXe4U8jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is only needed for updating the Github repo."
      ],
      "metadata": {
        "id": "LOjLINHKuiry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --version\n",
        "!pip install pip --upgrade\n",
        "!pip --version"
      ],
      "metadata": {
        "id": "K4clPijNcxbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMEZN5rya9eq"
      },
      "outputs": [],
      "source": [
        "# %pip install langchain\n",
        "# %pip install requests\n",
        "# %pip install weaviate-client\n",
        "# %pip install sentence_transformers\n",
        "%pip install openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZFseHuKa9eq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Weaviate\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to add from the google user secrets data the values for the \"WEAVIATE_OPENAI_API_KEY\" or we get it directly from the local environment variables."
      ],
      "metadata": {
        "id": "nr_vq_akTC4w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxm4yu3wa9er"
      },
      "outputs": [],
      "source": [
        "if Running_locally:\n",
        "  OPENAI_API_KEY = os.environ[\"WEAVIATE_OPENAI_API_KEY\"]\n",
        "elif RunningInCOLAB:\n",
        "  OPENAI_API_KEY = userdata.get(\"WEAVIATE_OPENAI_API_KEY\")\n",
        "else:\n",
        "  print(\"please extend coding to cover this as well\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yANgF7kUa9es"
      },
      "source": [
        "## Preparation\n",
        "As a preparation step, you need to prepare a vector database as an external knowledge source that holds all additional information. This vector database is populated by following these steps:\n",
        "\n",
        "- Collect and load your data\n",
        "- Chunk your documents\n",
        "- Embed and store chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okgWGmJpa9es"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "    f.write(res.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQTUm8lGa9es"
      },
      "outputs": [],
      "source": [
        "\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPVfKqGfa9es"
      },
      "source": [
        "Next, chunk your documents — Because the Document, in its original state, is too long to fit into the LLM’s context window, you need to chunk it into smaller pieces. LangChain comes with many built-in text splitters for this purpose. For this simple example, you can use the CharacterTextSplitter with a chunk_size of about 500 and a chunk_overlap of 50 to preserve text continuity between the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyRdvjyTa9et"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq3QHcXGa9et"
      },
      "source": [
        "Lastly, embed and store the chunks — To enable semantic search across the text chunks, you need to generate the vector embeddings for each chunk and then store them together with their embeddings. To generate the vector embeddings, you can use the OpenAI embedding model, and to store them, you can use the Weaviate vector database. By calling .from_documents() the vector database is automatically populated with the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOgnfg-wa9et"
      },
      "outputs": [],
      "source": [
        "client = weaviate.Client(embedded_options=EmbeddedOptions())\n",
        "vector_store = Weaviate.from_documents(\n",
        "    client = client,\n",
        "    documents = chunks,\n",
        "    embedding = HuggingFaceBgeEmbeddings(),\n",
        "    by_text = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpYSN36Pa9et"
      },
      "source": [
        "## Step 1: Retrieve\n",
        "Once the vector database is populated, you can define it as the retriever component, which fetches the additional context based on the semantic similarity between the user query and the embedded chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4Et-5Hna9et"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever()\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SWpbWOa9eu"
      },
      "source": [
        "## Step 2: Augment\n",
        "Next, to augment the prompt with the additional context, you need to prepare a prompt template. The prompt can be easily customized from a prompt template, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4em-MjWa9eu"
      },
      "outputs": [],
      "source": [
        "\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_GOqqhka9eu"
      },
      "source": [
        "## Step 3: Generate\n",
        "Finally, you can build a chain for the RAG pipeline, chaining together the retriever, the prompt template and the LLM. Once the RAG chain is defined, you can invoke it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrC5xs8Na9eu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git add -A\n",
        "# !git commit -m \"eliminating a security issue\"\n",
        "# !git push"
      ],
      "metadata": {
        "id": "3gNdMldN0gzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}